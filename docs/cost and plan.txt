AWS Infrastructure Analytics Pipeline
Complete Implementation Guide (Under $2/Month)

Project Overview: Build an end-to-end data pipeline for infrastructure monitoring and log analytics using AWS "Always Free" services + minimal S3 storage costs.

Target Role: L4 Data Engineer at Amazon  
Relevant Departments: AWS Infrastructure Operations, OpsTech, Data Center Engineering  
Monthly Cost: $0.50 - $2.00  
Timeline: 6-8 weeks  
Skill Level: Beginner to Intermediate

Table of Contents
Project Architecture
Prerequisites & Setup
Cost Breakdown
Phase 1: AWS Account Setup
Phase 2: Data Generation Pipeline
Phase 3: Data Processing & Storage
Phase 4: Monitoring & Alerting
Phase 5: Analytics & Visualization
Phase 6: Infrastructure as Code
Testing & Validation
Portfolio Presentation
Interview Preparation

1. Project Architecture

High-Level Architecture

Data Flow
Generation: Lambda function generates synthetic infrastructure logs every 5 minutes
Storage: Logs stored in S3 with date partitioning (year/month/day/hour)
Cataloging: Glue Crawler automatically discovers schema and creates tables
Processing: Lambda functions aggregate and transform data
Querying: Athena enables SQL queries on S3 data
Visualization: CloudWatch dashboards display key metrics
Alerting: CloudWatch alarms trigger SNS notifications for anomalies

2. Prerequisites & Setup

Required Software (All FREE)
AWS Account
• Existing AWS account (your free tier has expired, but we'll use Always Free services)
• Valid payment method (for minimal S3 charges)
Local Development Environment

Install Python 3.9+
• Windows: Download from python.org
• Mac: brew install python3
• Linux: sudo apt-get install python3 python3-pip

Install AWS CLI

Verify installation:

Install Terraform (Infrastructure as Code)

Verify installation:

Install Git

Install Visual Studio Code (Recommended IDE)
• Download from: code.visualstudio.com
• Install extensions:
• Python
• AWS Toolkit
• Terraform
• YAML
Python Libraries

AWS Account Configuration
Create IAM User for Development

Go to AWS Console → IAM → Users → Create User

User Details:
• Username: data-pipeline-dev
• Access type: Programmatic access

Attach Policies:
• AmazonS3FullAccess
• AWSLambda_FullAccess
• CloudWatchFullAccess
• AWSGlueConsoleFullAccess
• AmazonSNSFullAccess
• AmazonAthenaFullAccess
• IAMReadOnlyAccess

Save credentials:
• Access Key ID
• Secret Access Key
Configure AWS CLI

Verify configuration:

3. Cost Breakdown

Monthly Cost Estimate

| Service | Free Tier | Your Usage | Cost/Month |
|---------|-----------|------------|------------|
| Lambda | 1M requests, 400K GB-seconds | ~50K requests | $0.00 |
| S3 Storage | None (expired) | 20 GB | $0.46 |
| S3 Requests | None (expired) | 10K PUT, 50K GET | $0.05 |
| Glue Data Catalog | 1M objects | 1K objects | $0.00 |
| Athena | None | 1 GB scanned | $0.005 |
| CloudWatch | 10 metrics, 10 alarms | 5 metrics, 5 alarms | $0.00 |
| SNS | 1K emails | 100 emails | $0.00 |
| Data Transfer | 1 GB out | 0.5 GB | $0.00 |
| TOTAL | | | $0.51 |

Cost Optimization Strategies
S3 Lifecycle Policies
• Delete logs older than 30 days
• Reduces storage from 20GB to ~5GB
• Savings: $0.35/month
Athena Query Optimization
• Use partitioning (already implemented)
• Limit SELECT columns
• Use LIMIT clauses
• Savings: Minimal queries = $0.005/month
Lambda Optimization
• Reduce memory allocation (128MB sufficient)
• Optimize execution time
• Already FREE (within Always Free limits)

Optimized Monthly Cost: $0.15 - $0.50

4. Phase 1: AWS Account Setup (Day 1)

Step 1: Create S3 Bucket for Data Lake

Using AWS Console:
Go to S3 Console: https://s3.console.aws.amazon.com/
Click "Create bucket"
Bucket name: infrastructure-logs-[your-initials]-[random-number]
• Example: infrastructure-logs-jd-12345
• Must be globally unique
Region: us-east-1 (N. Virginia)
Block Public Access: Keep all boxes checked (security best practice)
Bucket Versioning: Disabled (to save costs)
Encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3)
Click "Create bucket"

Using AWS CLI:

Step 2: Create S3 Lifecycle Policy

Purpose: Automatically delete logs older than 30 days to minimize costs

Using AWS Console:
Go to your bucket → Management tab
Click "Create lifecycle rule"
Rule name: delete-old-logs
Rule scope: Apply to all objects
Lifecycle rule actions: Check "Expire current versions of objects"
Days after object creation: 30
Click "Create rule"

Using AWS CLI:

Step 3: Create SNS Topic for Alerts

Using AWS Console:
Go to SNS Console: https://console.aws.amazon.com/sns/
Click "Topics" → "Create topic"
Type: Standard
Name: infrastructure-alerts
Click "Create topic"
Click "Create subscription"
Protocol: Email
Endpoint: your-email@example.com
Click "Create subscription"
Check your email and confirm subscription

Using AWS CLI:

Step 4: Create IAM Role for Lambda

Using AWS Console:
Go to IAM Console → Roles → Create role
Trusted entity: AWS service → Lambda
Permissions policies: Attach these policies:
• AWSLambdaBasicExecutionRole
• AmazonS3FullAccess (or create custom policy with limited S3 access)
• CloudWatchFullAccess
• AmazonSNSFullAccess
Role name: lambda-data-pipeline-role
Click "Create role"

Using AWS CLI:

5. Phase 2: Data Generation Pipeline (Days 2-3)

Overview

We'll create a Lambda function that generates realistic infrastructure logs every 5 minutes. This simulates data from servers, applications, and network devices.

Lambda Function: Log Generator

File: lambdaloggenerator.py

See the separate artifact "Lambda Data Generation Scripts" for the complete code.

Deployment Steps
Create deployment package:
Deploy Lambda function:

Using AWS Console:
Go to Lambda Console: https://console.aws.amazon.com/lambda/
Click "Create function"
Function name: infrastructure-log-generator
Runtime: Python 3.9
Architecture: x86_64
Execution role: Use existing role → lambda-data-pipeline-role
Click "Create function"
In "Code" tab, click "Upload from" → ".zip file"
Upload lambda-log-generator.zip
Click "Save"
Go to "Configuration" → "Environment variables"
Add variable:
• Key: S3_BUCKET
• Value: infrastructure-logs-jd-12345 (your bucket name)
Click "Save"

Using AWS CLI:
Create EventBridge Rule (Schedule)

Using AWS Console:
Go to EventBridge Console → Rules → Create rule
Name: log-generator-schedule
Rule type: Schedule
Schedule pattern: Rate expression → rate(5 minutes)
Target: Lambda function → infrastructure-log-generator
Click "Create"

Using AWS CLI:
Test the function:

Expected Output:

6. Phase 3: Data Processing & Storage (Days 4-5)

Step 1: Create Glue Database

Using AWS Console:
Go to Glue Console → Databases → Add database
Database name: infrastructurelogsdb
Click "Create"

Using AWS CLI:

Step 2: Create Glue Crawler

Using AWS Console:
Go to Glue Console → Crawlers → Add crawler
Crawler name: infrastructure-logs-crawler
Data source: S3 → s3://your-bucket-name/logs/
IAM role: Create new role → AWSGlueServiceRole-logs
Schedule: On demand (we'll run manually)
Database: infrastructurelogsdb
Table prefix: raw_
Click "Create"

Using AWS CLI:

Step 3: Run Crawler

Using AWS Console:
Go to Glue Console → Crawlers
Select infrastructure-logs-crawler
Click "Run crawler"
Wait 2-3 minutes for completion

Using AWS CLI:

Step 4: Verify Table Creation

Using AWS Console:
Go to Glue Console → Tables
You should see: raw_logs
Click on table to view schema

Using AWS CLI:

7. Phase 4: Monitoring & Alerting (Days 6-7)

Step 1: Create CloudWatch Dashboard

Using AWS Console:
Go to CloudWatch Console → Dashboards → Create dashboard
Dashboard name: Infrastructure-Monitoring
Add widgets:

Widget 1: Log Volume (Line Chart)
• Data source: Logs Insights
• Query:

Widget 2: Error Rate (Number)
• Metric: Custom metric (we'll create this)
• Statistic: Sum

Widget 3: Lambda Invocations (Line Chart)
• Namespace: AWS/Lambda
• Metric: Invocations
• Dimension: FunctionName = infrastructure-log-generator

Using AWS CLI:

Step 2: Create CloudWatch Alarms

Alarm 1: High Error Rate

Using AWS Console:
CloudWatch → Alarms → Create alarm
Metric: Custom metric (we'll publish from Lambda)
Threshold: > 10 errors in 5 minutes
Action: Send notification to SNS topic infrastructure-alerts

Using AWS CLI:

Alarm 2: Lambda Failures

8. Phase 5: Analytics & Visualization (Days 8-10)

Step 1: Query Data with Athena

Setup Athena:
Go to Athena Console
Settings → Query result location: s3://your-bucket-name/athena-results/
Click "Save"

Query 1: Total Log Count

Query 2: Logs by Severity

Query 3: Top Error Messages

Query 4: Hourly Log Volume Trend

Query 5: Server Performance Metrics

Step 2: Create Saved Queries

Save these queries in Athena for reuse:
Click "Saved queries" → "Create"
Name each query appropriately
Save for future use

Step 3: Export Results

9. Phase 6: Infrastructure as Code (Days 11-12)

Terraform Configuration

File: main.tf

Deploy with Terraform

```bash
Initialize Terraform
terraform init

Create terraform.tfvars
cat > terraform.tfvars << 'EOF'
your_email = "your-email@example.com"
EOF

Plan deployment
terraform plan